{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb14ce-488a-4a96-a00f-70db76abf403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db870c8-8591-4594-bbb8-0be93eb14826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import models\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int\n",
    "                 ):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv(x)\n",
    "\n",
    "# U-Net model\n",
    "class unet(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]\n",
    "    ):\n",
    "        super(unet, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNet\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature * 2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature * 2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "        # self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[int(idx / 2)]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                # print(f'x.shape: {x.shape}, skip_connection.shape: {skip_connection.shape}')\n",
    "                skip_connection = TF.resize(skip_connection, size=x.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "    \n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "# U-Net model with ResNet18 base\n",
    "class unet_pre_res18(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = models.resnet18(pretrained)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3])  # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5])  # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.align_input = convrelu(in_channels, 3, 3, 1)\n",
    "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.align_input(input)\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "\n",
    "        x = self.upsample(layer4)\n",
    "        # print(x)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        if x.shape != layer3.shape:\n",
    "            x = TF.resize(x, layer3.shape[2:])\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        if x.shape != layer2.shape:\n",
    "            x = TF.resize(x, layer2.shape[2:])\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        if x.shape != layer1.shape:\n",
    "            x = TF.resize(x, layer1.shape[2:])\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        if x.shape != layer0.shape:\n",
    "            x = TF.resize(x, layer0.shape[2:])\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        if x.shape != x_original.shape:\n",
    "            x = TF.resize(x, x_original.shape[2:])\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "        # print(\"\\tIn Model: input size\", input.size(),\n",
    "                # \"output size\", out.size())\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel, stride):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(\n",
    "            input_dim, output_dim, kernel_size=kernel, stride=stride\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class ResidualConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, stride, padding, BatchNorm):\n",
    "        super(ResidualConv, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            BatchNorm(input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                input_dim, output_dim, kernel_size=3, stride=stride, padding=padding\n",
    "            ),\n",
    "            BatchNorm(output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.conv_skip = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=stride, padding=1),\n",
    "            BatchNorm(output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.conv_block(x) + self.conv_skip(x)\n",
    "\n",
    "# Residual U-Net model\n",
    "class res_unet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, filters=[64, 128, 256, 512]):\n",
    "        super(res_unet, self).__init__()\n",
    "\n",
    "        BatchNorm = nn.BatchNorm2d\n",
    "\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, filters[0], kernel_size=3, padding=1),\n",
    "            BatchNorm(filters[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.input_skip = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, filters[0], kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.residual_conv_1 = ResidualConv(filters[0], filters[1], 2, 1, BatchNorm)\n",
    "        self.residual_conv_2 = ResidualConv(filters[1], filters[2], 2, 1, BatchNorm)\n",
    "\n",
    "        self.bridge = ResidualConv(filters[2], filters[3], 2, 1, BatchNorm)\n",
    "\n",
    "        self.upsample_1 = Upsample(filters[3], filters[3], 2, 2)\n",
    "        self.up_residual_conv1 = ResidualConv(filters[3] + filters[2], filters[2], 1, 1, BatchNorm)\n",
    "\n",
    "        self.upsample_2 = Upsample(filters[2], filters[2], 2, 2)\n",
    "        self.up_residual_conv2 = ResidualConv(filters[2] + filters[1], filters[1], 1, 1, BatchNorm)\n",
    "\n",
    "        self.upsample_3 = Upsample(filters[1], filters[1], 2, 2)\n",
    "        self.up_residual_conv3 = ResidualConv(filters[1] + filters[0], filters[0], 1, 1, BatchNorm)\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Conv2d(filters[0], out_channels, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, gt=None):\n",
    "        # Encoder\n",
    "        x1 = self.input_layer(x) + self.input_skip(x)\n",
    "        x2 = self.residual_conv_1(x1)\n",
    "        x3 = self.residual_conv_2(x2)\n",
    "        # Bridge\n",
    "        x4 = self.bridge(x3)\n",
    "        # Decoder\n",
    "        x4 = self.upsample_1(x4)\n",
    "        x5 = torch.cat([x4, x3], dim=1)\n",
    "\n",
    "        x6 = self.up_residual_conv1(x5)\n",
    "\n",
    "        x6 = self.upsample_2(x6)\n",
    "        x7 = torch.cat([x6, x2], dim=1)\n",
    "\n",
    "        x8 = self.up_residual_conv2(x7)\n",
    "\n",
    "        x8 = self.upsample_3(x8)\n",
    "        x9 = torch.cat([x8, x1], dim=1)\n",
    "\n",
    "        x10 = self.up_residual_conv3(x9)\n",
    "\n",
    "        output = self.output_layer(x10)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, BatchNorm):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            BatchNorm(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            BatchNorm(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out, BatchNorm):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            BatchNorm(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int, BatchNorm):\n",
    "        super(Attention_block, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            BatchNorm(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            BatchNorm(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            BatchNorm(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "\n",
    "        diffY = x1.size()[2] - g1.size()[2]\n",
    "        diffX = x1.size()[3] - g1.size()[3]\n",
    "\n",
    "        g1 = F.pad(g1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "class att_unet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, sync_bn=True):\n",
    "        super(att_unet, self).__init__()\n",
    "\n",
    "        BatchNorm = nn.BatchNorm2d\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(in_channels, 64, BatchNorm)\n",
    "        self.Conv2 = conv_block(64, 128, BatchNorm)\n",
    "        self.Conv3 = conv_block(128, 256, BatchNorm)\n",
    "        self.Conv4 = conv_block(256, 512, BatchNorm)\n",
    "        self.Conv5 = conv_block(512, 1024, BatchNorm)\n",
    "\n",
    "        self.Up5 = up_conv(1024, 512, BatchNorm)\n",
    "        self.Att5 = Attention_block(512, 512, 256, BatchNorm)\n",
    "        self.Up_conv5 = conv_block(1024, 512, BatchNorm)\n",
    "\n",
    "        self.Up4 = up_conv(512, 256, BatchNorm)\n",
    "        self.Att4 = Attention_block(256, 256, 128, BatchNorm)\n",
    "        self.Up_conv4 = conv_block(512, 256, BatchNorm)\n",
    "\n",
    "        self.Up3 = up_conv(256, 128, BatchNorm)\n",
    "        self.Att3 = Attention_block(128, 128, 64, BatchNorm)\n",
    "        self.Up_conv3 = conv_block(256, 128, BatchNorm)\n",
    "\n",
    "        self.Up2 = up_conv(128, 64, BatchNorm)\n",
    "        self.Att2 = Attention_block(64, 64, 32, BatchNorm)\n",
    "        self.Up_conv2 = conv_block(128, 64, BatchNorm)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x, gt=None):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5, x=x4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=x3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=x2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=x1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2787e5-8188-4dc8-8d23-f5c8df29720d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
